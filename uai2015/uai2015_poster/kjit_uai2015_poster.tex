 \documentclass[25pt, a0paper, portrait, margin=0mm, innermargin=10mm,
     blockverticalspace=7mm, colspace=7mm, subcolspace=8mm]{tikzposter} %Default values for poster format options.

 \tikzposterlatexaffectionproofoff % not show small comment on how the poster
 %was made at bottom of poster

\usepackage{url}
\usepackage{algorithmic}
\usepackage{algorithm}

\renewcommand{\algorithmicrequire}{\textbf{In:}}
\renewcommand{\algorithmicensure}{\textbf{Out:}}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
% make the itemize compact
\setlist[itemize]{nosep}
\setlist[enumerate]{itemsep=2mm}
%\usepackage{caption}
\usepackage[mathscr]{eucal}
\usepackage{graphicx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.

\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{empheq}

\usepackage{lmodern}
\usepackage{pifont}
\usepackage{relsize}
\usepackage{setspace}
\usepackage{tikz}
%\usetikzlibrary{bayesnet}
\usetikzlibrary{shapes.geometric,decorations.markings}
\usepackage{xcolor}

%\usepackage{epstopdf}
%\usepackage{auto-pst-pdf}
%\usepackage{subfig}


\newcommand{\diag}{\mathop{\mathrm{diag}}}
\newcommand{\trace}{\mathop{\mathrm{tr}}}
\newcommand{\median}{\mathop{\mathrm{median}}}
%\newcommand{\proj}{\mathop{\mathrm{proj}}}


\newcommand{\bx}{\mathbf{x}}				% all variables
%\newcommand{\factor}{\psi}				% factor
\newcommand{\factor}{f}				% AG: made this f for consistency
\newcommand{\outV}{V}                         %AG: output variable of a factor
\newcommand{\fis}[1]{\mathrm{ne}(#1)}   	% index set for variables connected to  factor
\newcommand{\fx}[1]{ \mathbf{x}_{\mathrm{ne}(#1)} }   	% variables of a factor
\newcommand{\xin}{\mathbf{x}_{ \mathrm{in} }} 			% parents of directed factor
\newcommand{\xout}{\mathbf{x}_{ \mathrm{out} }}			% child of directed factor
\newcommand{\msg}[2]{m_{#1 \rightarrow #2}}			% message from arg1 to arg2
\newcommand{\approxMsg}[3]{M_{#1 \rightarrow #2}^{#3}}			% message from arg1 to arg2
\newcommand{\uncert}{{\mathfrak v}}          %%AG: variable used to denote uncertainty
\newcommand{\uncertaintyMsg}[3]{{\mathfrak V}_{#1 \rightarrow #2}^{#3}}			% message from arg1 to arg2
\newcommand{\diffd}{\mathrm{d}}
%\newcommand{\proj}{\mathrm{proj}}   %AG: replaced with notation using less space

%\newcommand{\proj}{\mathbb{Q}}


\newcommand{\projP}[1]{\mathrm{proj} \left [ #1 \right]}   
\DeclareMathOperator*{\proj}{\text{proj}} % WJ defined this

\newcommand{\argmin}[1]{\mathrm{arg}\mathrm{min}_{#1}}
%\newcommand{\kld}[2]{\mathrm{KL} \left [ #1 || #2 \right ]}
%\newcommand{\expectationE}[2]{ \mathbb{E}_{#2}  \left[ #1 \right] }

% random features stuff
\newcommand{\feax}{\mathsf{x}}
\newcommand{\feaX}{\mathsf{X}}
\newcommand{\feay}{\mathsf{y}}
\newcommand{\feaY}{\mathsf{Y}}

\newcommand{\figref}[1]{Fig.~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\tabref}[1]{Table.~\ref{#1}}

\everymath{\color{mathcolor}}

% Gatsby Unit, University College London$^1$ \\
% \vspace*{2mm}
% University of Oxford$^2$ \\
% \texttt{ \{wittawatj,  arthur.gretton\}@gmail.com}, \, \texttt{nheess@nhuk.de} \\ 
% \texttt{ali@arkitus.com}, \, \texttt{balaji@gatsby.ucl.ac.uk},\\
% \texttt{dino.sejdinovic@gmail.com},\, \texttt{z.szabo@ucl.ac.uk}


\graphicspath{{../img/}}
%%%%%%%%%%%%%%
\title{Kernel-Based Just-In-Time Learning For Passing Expectation Propagation Messages  }
%\title{Kernel-Based Just-In-Time Learning For Expectation Propagation }
    \author{\hspace{37mm} Wittawat Jitkrittum,$^1$ \, Arthur Gretton,$^1$ \, Nicolas Heess, \, S. M. Ali Eslami \, \newline
Balaji Lakshminarayanan,$^1$ \, Dino Sejdinovic$^2$ and \, Zolt{\'a}n Szab{\'o}$^1$  
}
\institute{%
Gatsby Unit, University College London$^1$ 
\hspace{3cm}
University of Oxford$^2$
%\newline Google DeepMind$^2$
}

% -- PREDEFINED THEMES ---------------------- %
% Choose LAYOUT:  Default, Basic, Rays, Simple, Envelope, Wave, Board, Autumn, Desert,
\usetheme{Board}
%\usenotestyle{Corner}
\usenotestyle{Sticky}
\colorlet{contactnotebgcolor}{yellow!30!white}
\colorlet{highlightnotebgcolor}{green!15!white}

\colorlet{notebgcolor}{contactnotebgcolor}
\colorlet{notefrcolor}{notebgcolor}
\colorlet{blockbodybgcolor}{white}
%\usetheme{Autumn}
%\usetitlestyle{Default}
%\useblockstyle{Minimal}
%\colorlet{titlefgcolor}{green}
%\usecolorstyle[colorPalette=BrownBlueOrange]{Germany}
%\usecolorstyle[colorPalette=BlueGrayOrange]{Spain}
%\usecolorstyle{Australia}
%\usecolorstyle{Britain}
%\usecolorstyle{Sweden}
%\usecolorstyle{Russia}

\definetitlestyle{MyTitleStyle}{
   width=0.97\paperwidth, roundedcorners=0, linewidth=0pt, innersep=8mm,
   titletotopverticalspace=5mm, titletoblockverticalspace=5mm,
   titlegraphictotitledistance=0pt, titletextscale=1
}{}
\usetitlestyle{MyTitleStyle}

\settitle{ \centering \vbox{
\@titlegraphic \\[\TP@titlegraphictotitledistance] \centering
\color{titlefgcolor} \vspace{1.3em} {\fontsize{54}{42} \textbf{\@title} \par}
%\color{titlefgcolor} {\huge \textbf{\@title} \par}
\vspace*{1em}
{\huge \@author \par} \vspace*{1em} {\LARGE \@institute}
}}


\colorlet{mathcolor}{blue!40!black}
% \newcommand{\Smile}{\Simley{0.6}{green!20}}
% \newcommand{\Frown}{\Simley{-0.53}{red!15}}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\Smile}{ {\large \textcolor{green}{\cmark}} }
\newcommand{\Frown}{ {\large \textcolor{red}{\xmark}} }

\newcommand{\Vskip}{\vspace{5mm}}
%\newenvironment{wideitemize}{\itemize\addtolength{\itemsep}{100pt}}{\enditemize}

% make body text's font bigger.
% compile font_gen.tex first.
% http://tex.stackexchange.com/questions/243678/how-to-make-tikzposter-work-with-font-sizes-larger-than-25pt
\makeatletter
%\input{theguy36pt.clo}
\input{theguy28pt.clo}
\makeatother 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle[width=0.95\paperwidth ]
\begin{columns}
%==================================
\column{0.5}
%==================================
%--------------------------
\block[roundedcorners=40]{Introduction}{
%--------------------------
EP is a widely used message passing based inference algorithm. 
\begin{itemize}
    \item \textbf{Problem}: Expensive to compute outgoing from incoming messages.
    \item \textbf{Goal}: Speed up computation by a cheap regression function
        (message operator): 
        \textcolor{mathcolor}{
    \begin{equation*}
        \text{incoming messages} \mapsto \text{outgoing message}.
    \end{equation*}
    }
\end{itemize}
%
\vspace*{-15mm}
\textbf{Merits}:
\begin{itemize}
    \item Efficient online update of the operator during inference.
    \item Uncertainty monitored to invoke new training examples when needed. 
    \item Automatic random feature representation of incoming messages.
\end{itemize}
}

%--------------------------
\block{Expectation Propagation (EP)}{
%--------------------------
%\definecolor{mygreen}{RGB}{0,100,0}
\colorlet{mygreen}{green!52!black}

Under an approximation that each factor fully factorizes, an outgoing EP
message $\textcolor{mygreen}{m_{f\rightarrow V_i}}$ takes the form
 
\vspace{-5mm}
\begin{tikzfigure}[]
    \includegraphics[width=37cm]{img/inout_msg-crop.pdf}
\end{tikzfigure}

\vspace{-5mm}

\begin{tikzfigure}[]
    \vspace{-10mm}
%\centering
 %\subfloat[ Message passing on a factor graph.]{
 \includegraphics[width=17cm]{img/factor_graph-crop.pdf}
 %}
 \hspace{1cm}
 %
 %\subfloat[ Projection of $r_{f\rightarrow V}$ to a Gaussian]{
 \includegraphics[width=17cm]{img/proj_demo-crop.pdf}
 %}
 %
\end{tikzfigure}
%
\textbf{Projected message:} 

\begin{itemize}
    \item $q_{f\rightarrow V}(v)=\proj\left[r_{f\rightarrow V}(v)\right] \in
        \text{ExpFam}$ with sufficient statistic $u(v)$.

    \item Compute $q_{f\rightarrow V}(v)$ by moment matching: $\mathbb{E}_{q_{f\rightarrow
        V}}\left[u(v)\right]=\mathbb{E}_{r_{f\rightarrow V}}\left[u(v)\right] $.
\end{itemize}

%Let $q_{f\rightarrow V}(v)\in\text{ExpFam}$
%with sufficient statistic $u(v)$. 


%\begin{equation*}
%\mathbb{E}_{q_{f\rightarrow V}}\left[T(v)\right]=\mathbb{E}_{r_{f\rightarrow V}}\left[T(v)\right] \text{(moment matching).}
%\end{equation*}
%
} % end block


%--------------------------
\block{Kernel on Incoming Messages}{
%--------------------------

Propose to incrementally learn during inference a kernel-based EP message operator 
(distribution-to-distribution regression) 
%
\textcolor{mathcolor}{
\begin{equation*}
    \left[ m_{V_j \rightarrow f}  \right]_{j=1}^c \mapsto q_{f \rightarrow V_i},
\end{equation*}
}
%
for any factor $f$ that can be sampled. 

\begin{itemize}
    \item Product distribution of $c$ incoming messages: 
        $\mathsf{r}:=\times_{l=1}^c r_{l}$, \hspace{5mm}
        $\mathsf{s}:=\times_{l=1}^c s_{l}$.     
    
    \item Mean embedding of $\mathsf{r}$:
        $\mu_{\mathsf{r}}:=\mathbb{E}_{a \sim \mathsf{r}}k(\cdot,a)$.

    \item Gaussian kernel on (product) distributions. Two-staged random feature
        approx.:
        %\begin{align*}
%&= \exp\left(-\frac{1}{2\gamma^{2}}\left\langle
%  \mu_{\mathsf{r}},\mu_{\mathsf{r}}\right\rangle
%  +\frac{1}{\gamma^{2}}\left\langle
%  \mu_{\mathsf{r}},\mu_{\mathsf{s}}\right\rangle
%  -\frac{1}{2\gamma^{2}}\left\langle
%  \mu_{\mathsf{s}},\mu_{\mathsf{s}}\right\rangle \right).
        %\end{align*}

        \textcolor{mathcolor}{
\begin{equation*}
       \kappa(\mathsf{r}, \mathsf{s}) = 
        \exp\left(-\frac{\|\mu_{\mathsf{r}}-
        \mu_{\mathsf{s}}\|_{\mathcal{H}}^{2}}{2\gamma^{2}}\right)
%\kappa(\mathsf{r}, \mathsf{s}) 
        \stackrel{\textcolor{red}{1^{st}}}{\approx}\exp\left(-\frac{
    \|\hat{\phi}(\mathsf{r})-\hat{\phi}(\mathsf{s})\|_{D_\mathrm{in}}^{2}}{2\gamma^{2}}\right)
    \stackrel{\textcolor{red}{2^{nd}}}{\approx}
    \hat{\psi}(\mathsf{r})^\top \hat{\psi}(\mathsf{s}).
\end{equation*}
}

\end{itemize}
%
\vspace{-5mm}
} % end block

%--------------------------
\block{\hspace{-12mm} Message Operator: Bayesian Linear Regression }{
%--------------------------

\begin{itemize}
    \item \textbf{Input:} $\mathsf{X}=\left(\mathsf{x}_{1}|\cdots|\mathsf{x}_{N}\right)$: 
        $N$ training incoming messages represented as random feature vectors.
    \item \textbf{Output:} $\mathsf{Y}=\left(\mathbb{E}_{r_{\factor\rightarrow
        \outV}^{1}}u(v)|\cdots|\mathbb{E}_{r_{f\rightarrow
        \outV}^{N}}u(v)\right)\in\mathbb{R}^{D_{y}\times N}$: 
        expected sufficient statistics of outgoing messages.
    \item Inexpensive online updates of posterior mean and covariance.
    \item Bayesian regression gives prediction and predictive variance.
    \item If predictive variance $>$ threshold, query the importance sampling oracle.
\end{itemize}
%\textcolor{red}{code available}

}% end block


 \note[targetoffsetx=-10.8cm, targetoffsety=-7.5cm,rotate=0,
     angle=270,radius=0cm, width=19cm,innersep=1cm ]{
        Contact: \url{wittawat@gatsby.ucl.ac.uk}.
 } % end note

 \note[targetoffsetx=15cm, targetoffsety=-7.5cm,rotate=0,
     angle=270,radius=0cm, width=29cm,innersep=1cm ]{
         Code download:  
         \url{http://github.com/wittawatj/kernel-ep}.
 } % end note

\colorlet{notebgcolor}{orange!10}
\colorlet{notefrcolor}{orange!10}
 \note[targetoffsetx=46.5cm, targetoffsety=-7.5cm,rotate=0,
     angle=270,radius=0cm, width=31cm,innersep=1cm]{
We thank the Gatsby Charitable Foundation for the financial support.
 } % end note
%==================================
\column{0.5}
%==================================

%--------------------------
\block{Two-Staged Random Features }{
%--------------------------
% \begin{algorithm}[t]
% \caption{Construction of two-stage random features for $\kappa$}
% \label{algo:random_features_kgg}
    \begin{spacing}{1.3}
\begin{algorithmic}[1]
\REQUIRE 
%Joint $\mathsf{r}$ of incoming messages, 
$\mathscr{F}(k)$: Fourier transform of $k$, $D_\mathrm{in}$: \#inner features, 
$D_\mathrm{out}$: \#outer features, $k_\text{gauss}$: Gaussian kernel on
$\mathbb{R}^{D_\mathrm{in}}$
%, outer Gaussian width $\gamma^2$.
\ENSURE Random features $\hat{\psi}(\mathsf{r}) \in \mathbb{R}^{D_\mathrm{out}}$

%\STATE Compute the Fourier transform $\hat{k}$ of the kernel $k$.
\STATE Sample  $\{ \omega_i \}_{i=1}^{D_\mathrm{in}} \overset{i.i.d}{\sim} \mathscr{F}(k)$,
\hspace{2cm}
$\{b_i\}_{i=1}^{D_\mathrm{in}} \overset{i.i.d}{\sim} U[0, 2\pi] $.

%\hspace{10mm} $\{b_i\}_{i=1}^{D_\mathrm{in}} \overset{i.i.d}{\sim} U[0, 2\pi] $.

%\STATE Sample $\{b_i\}_{i=1}^{D_\mathrm{in}} \overset{i.i.d}{\sim} U[0, 2\pi] $.

\STATE $\hat{\phi}(\mathsf{r}) = \sqrt{\frac{2}{D_\mathrm{in}}} \left(
\mathbb{E}_{x \sim \mathsf{r}} 
\cos(\omega_{i}^{\top}x+b_{i} ) \right)_{i=1}^{D_\mathrm{in}} \in \mathbb{R}^{D_\mathrm{in}}$ \\
%\STATE  $\hat{\phi}(p)=\mathbb{E}_{p(x)}\sqrt{\frac{2}{D_\mathrm{in}}}\left(\cos\left(\omega_{1}^{\top}x+b_{1}\right),\ldots,\cos\left(\omega_{D_\mathrm{in}}^{\top}x+b_{D_\mathrm{in}}\right)\right)^{\top}$.
%If $\mathsf{r}(x)=\mathcal{N}(x;m, \Sigma )$,  \\
%$\hat{\phi}( \mathsf{r}) =  \sqrt{\frac{2}{D_\mathrm{in}}} \left( \cos(\omega_{i}^{\top}m +b_{i}) \exp 
%\left(-\frac{1}{2}\omega_{i}^{\top}\Sigma \omega_{i} \right) \right)_{i=1}^{D_\mathrm{in}}$.
% \small
% \begin{equation*}
% \hat{\phi}( \mathsf{r}) = \sqrt{\frac{2}{D_\mathrm{in}}} \left( \cos(\omega_{i}^{\top}m +b_{i}) \exp 
% \left(-\frac{1}{2}\omega_{i}^{\top}\Sigma \omega_{i} \right) \right)_{i=1}^{D_\mathrm{in}}.
% \end{equation*}
%
%Even if $p$ is not a normal distribution, we may still use it as an approximation.
%
\STATE Sample $\{ \nu_i \}_{i=1}^{D_\mathrm{out}} \overset{i.i.d}{\sim}
\mathscr{F}({k}_{\text{gauss}}(\gamma^{2}))$, \hspace{2cm}
$\{c_i\}_{i=1}^{D_\mathrm{out}} \overset{i.i.d}{\sim} U[0, 2\pi] $.

%\hspace{10mm} 
%\STATE Sample $\{c_i\}_{i=1}^{D_\mathrm{out}} \overset{i.i.d}{\sim} U[0, 2\pi] $.

%i.e., Fourier transform of a Gaussian kernel with width $\gamma^2$.
\STATE $\hat{\psi}(\mathsf{r}) = \sqrt{\frac{2}{D_\mathrm{out}}} \left(  
\cos(\nu_{i}^{\top} \hat{\phi}(\mathsf{r}) + c_{i} ) \right)_{i=1}^{D_\mathrm{out}} \in 
\mathbb{R}^{D_\mathrm{out}}$
\end{algorithmic}
\end{spacing}
% \end{algorithm}

\vspace{-8mm}
} % end block

%--------------------------
\block{ Experiment 1: Uncertainty Estimates}{
%--------------------------
%\vspace{-8mm}
\hspace{-14mm}
\begin{minipage}{23cm}
\begin{center}
%\includegraphics[width=20cm]{binary_logistic_regression-crop}
\includegraphics[width=21cm]{img/binlog_graph_title-crop}
\end{center}
\end{minipage}
%
\begin{minipage}{19cm}
    \begin{itemize}
        \item \textcolor{black}{Approx}.
            $f(p|z) = \delta\left(p-\frac{1}{1+\exp(-z)}\right)$. 
    %\vspace{-10mm}
        \item \textcolor{black}{Incoming messages: }
            %$\msg{z_i}{\factor} = \mathcal{N}(z_i; \mu, \sigma^2), \hspace{2cm}
            %\msg{p_i}{\factor} = \text{Beta}(p_i; \alpha, \beta). $
        \begin{align*}
        \msg{z_i}{\factor} &= \mathcal{N}(z_i; \mu, \sigma^2), \\
        \msg{p_i}{\factor} &= \text{Beta}(p_i; \alpha, \beta). 
        \end{align*}
    \end{itemize}
\end{minipage}
  
\vspace{10mm}

\begin{itemize}

\item Training messages collected from 20 EP runs on toy data.  
\item \#Random features: $D_{in} = 300$ and  $D_{out} = 500$.

\begin{tikzfigure}
%\centering
  %\subfloat[Parameters of $\msg{z_i}{\factor}$ \label{fig:logistic_uncertainty_test}]{
  \hspace{-10cm}
  \includegraphics[width=13cm]{uncertainty/logistic_uncertainty_test-crop}
  %}
  \hspace{1cm}
  %
  %\subfloat[Uncertainty estimates \label{fig:logistic_uncertainty_all}]{
      \includegraphics[width=13cm]{uncertainty/logistic_uncertainty_all-crop}
  %\hspace{1.5cm}
  %}
  %
  %\caption{   
  %Eslami et. al.'s random forests (KL-based agreement of predictions of different trees) 
  %on the two uncertainty test sets shown. For testing, we fix the other incoming message 
  %%$\msg{p_i}{\factor}$ to $\text{Beta}(p_i; 1, 2)$.
  %}

  %\label{fig:logistic_uncertainty}
\end{tikzfigure}
\end{itemize}
\vspace{-8mm}
} % end block

\colorlet{notebgcolor}{highlightnotebgcolor}
\colorlet{notefrcolor}{highlightnotebgcolor}
\note[targetoffsetx=6.9cm,targetoffsety=-1.5cm,innersep=0.8cm, 
  angle=-35,connection, width=13cm, radius=8.5cm]{
  KJIT gives smoother uncertainty estimates compared to random forests.
  }
  
%%--------------------------
%\block{ Experiment 2: Classification Errors}{
%%--------------------------

%% logistic temporal uncertainty
%% \begin{figure}[t]
%% \centering
%% \includegraphics[width=0.95\textwidth]{online/logistic_temporal_uncertainty-crop}
%% \caption{Uncertainty estimate of KJIT in its prediction of outgoing messages at each factor invocation,
%% for the binary logistic regression problem. The black dashed lines indicate the start 
%% of a new inference problem.
%% \label{fig:logistic_temporal_uncertainty}
%% }
%% \end{figure}

%    Fix true $\boldsymbol{w}$. Sequentially present 30 problems. Generate
%    $\{(x_i, y_i)\}_{i=1}^{300}$ for each.  
%        %
%\begin{tikzfigure}
%    %\vspace{5mm}
%  %\centering
%  \hspace{-10cm}
%  %\subfloat[Test error \label{fig:logistic_01_loss}]{
%  \includegraphics[width=13cm]{online/logistic_01_loss-crop}
%  %}
%  \hspace{2cm}
%  %\subfloat[Inference time\label{fig:logistic_inference_time}]{
%  \includegraphics[width=13cm]{online/logistic_inference_time-crop}
%  %}
%\end{tikzfigure}

%\textbf{Sampling + KJIT} = proposed KJIT with an importance sampling oracle.

%}% end block

%\colorlet{notebgcolor}{highlightnotebgcolor}
%\note[targetoffsetx=7cm,targetoffsety=0.7cm,innersep=0.8cm, 
%  angle=-10, width=10cm, radius=8cm, connection]{
%  Same classification errors as obtained by importance sampling; much faster.
%  }
  
%--------------------------
\block{ Experiment 2: Real Data}{
%--------------------------
\begin{itemize}
    \item Binary logistic regression.  Sequentially present 4 real datasets
        to the operator.
    \item Diverse distributions of incoming messages.
\end{itemize}

\vspace{-5mm}
% UCI data classification error
% Inference time
\begin{tikzfigure}
  %\centering
  %\subfloat[Incoming messages]{
  %\includegraphics[width=13.cm]{online/uci_in_msgs-crop}
  %}
  \hspace{-10cm}
  %\subfloat[Test error\label{fig:uci_01_loss}]{
  \includegraphics[width=13cm]{online/uci_classification-crop}
  %}
  %
  \hspace{1cm}
  %\subfloat[Inference time\label{fig:uci_infer_time}]{
  \includegraphics[width=13cm]{online/uci_infer_time-crop}
  %}
\end{tikzfigure}

\begin{itemize}
 \item \textbf{Sampling + KJIT} = proposed KJIT with an importance sampling oracle.
    \item KJIT operator can adapt to the change of input message distributions.
\end{itemize}

\vspace{-5mm}
% UCI datasets. temporal uncertainty.
\begin{tikzfigure}
\centering
\includegraphics[width=38cm]{online/uci_temporal_uncertainty-crop}
%\caption{
%Uncertainty estimate of KJIT for outgoing messages on the four UCI datasets.
%\label{fig:uci_temporal_uncertainty}
%}
\end{tikzfigure}

\vspace{-12mm}
} % end block

\colorlet{notebgcolor}{highlightnotebgcolor}
\colorlet{notefrcolor}{highlightnotebgcolor}
\note[targetoffsetx=6.9cm,targetoffsety=3cm,innersep=0.8cm, 
  angle=5,connection, width=11cm, radius=8cm]{
 Much faster with same classification errors as obtained by importance sampling.
  }

%--------------------------
\block{ Experiment 3: Compound Gamma Factor }{
%--------------------------
Infer posterior of the precision $\tau$ of $x \sim \mathcal{N}(x; 0, \tau^{-1})$ 
from observations $\{x_i\}_{i=1}^N $:
%

\vspace{5mm}

\begin{minipage}{21cm}
      \begin{align*}
         \textcolor{magenta}{r_2} &\sim \mathrm{Gamma}(r_2; s_1, r_1)  \\
         \tau &\sim \mathrm{Gamma}(\tau; s_2, \textcolor{magenta}{r_2} ) \\
         (s_1, r_1, s_2) &= (1, 1, 1).
      \end{align*}
\end{minipage}
%
\begin{minipage}{21cm}
  \includegraphics[width=11cm]{img/compound_gamma_graph-crop}
\end{minipage}


  %
%\begin{subcolumns}
%    \subcolumn{.5}
%      \begin{align*}
%         \textcolor{magenta}{r_2} &\sim \mathrm{Gamma}(r_2; s_1, r_1)  \\
%         \tau &\sim \mathrm{Gamma}(\tau; s_2, \textcolor{magenta}{r_2} ) \\
%         (s_1, r_1, s_2) &= (1, 1, 1).
%      \end{align*}
%  %
%    \subcolumn{.5}
%  \includegraphics[width=11cm]{img/compound_gamma_graph-crop}
%\end{subcolumns}
%
% compound gamma inferred results and time.
\begin{tikzfigure}
  \centering
% 	\subfloat[Posteriors]{
% 	\includegraphics[width=0.49\columnwidth]{img/online/cg_post_corr-crop}
% 	%\missingfigure[figwidth=0.49\columnwidth]{}
% 	}
  %
  %\subfloat[Inferred shape \label{fig:cg_infer_shape}]{
  \includegraphics[width=11cm]{online/cg_post_shape-crop}
  %\missingfigure[figwidth=0.49\columnwidth]{}
  %}
  %
  \hspace{1.8cm}
  %\subfloat[Inferred rate \label{fig:cg_infer_rate}]{
  \includegraphics[width=10.5cm]{online/cg_post_rate-crop}
  %\missingfigure[figwidth=0.49\columnwidth]{}
  %}
  %
  \hspace{1.8cm}
  %\subfloat[Inference time\label{fig:cg_infer_time}]{
  \includegraphics[width=11.2cm]{online/cg_inference_time-crop}
  %}
  %\caption{Shape (a) and rate (b) parameters of the inferred posteriors in 
  %the compound gamma problem. 
  %(c) KJIT is able to infer equally good posterior parameters compared to Infer.NET 
  %while requiring a runtime several orders of magnitude lower. }
\end{tikzfigure}


\begin{itemize}
    \item \textbf{Infer.NET + KJIT} = proposed KJIT with a hand-crafted factor as oracle.
    \item \textbf{Inference quality}: as good as hand-crafted factor; much faster.
\end{itemize}

} % end block


%%--------------------------
%\block{Acknowledgement}{
%%--------------------------
%%WJ, AG, BL, and ZSz 
%We thank the Gatsby Charitable Foundation for the financial support.
%} % end block 

\end{columns}



\end{document}

