% This file was created with JabRef 2.10b2.
% Encoding: UTF-8


@Article{Alvarez2011,
  Title                    = {Kernels for Vector-Valued Functions: a Review},
  Author                   = {Alvarez, Mauricio A. and Rosasco, Lorenzo and Lawrence, Neil D.},
  Journal                  = {{arXiv}:1106.6251 [cs, math, stat]},
  Year                     = {2011},

  Month                    = jun,
  Note                     = {{arXiv}: 1106.6251},

  Abstract                 = {Kernel methods are among the most popular techniques in machine learning. From a frequentist/discriminative perspective they play a central role in regularization theory as they provide a natural choice for the hypotheses space and the regularization functional through the notion of reproducing kernel Hilbert spaces. From a Bayesian/generative perspective they are the key in the context of Gaussian processes, where the kernel function is also known as the covariance function. Traditionally, kernel methods have been used in supervised learning problem with scalar outputs and indeed there has been a considerable amount of work devoted to designing and learning kernels. More recently there has been an increasing interest in methods that deal with multiple outputs, motivated partly by frameworks like multitask learning. In this paper, we review different methods to design or learn valid kernel functions for multiple outputs, paying particular attention to the connection between probabilistic and functional methods.},
  File                     = {arXiv\:1106.6251 PDF:/nfs/nhome/live/wittawat/.zotero/zotero/q6a3aco7.default/zotero/storage/S73W4DWS/Alvarez et al. - 2011 - Kernels for Vector-Valued Functions a Review.pdf:application/pdf;arXiv.org Snapshot:/nfs/nhome/live/wittawat/.zotero/zotero/q6a3aco7.default/zotero/storage/H93Z9ZW3/1106.html:text/html},
  Keywords                 = {Computer Science - Artificial Intelligence, Mathematics - Statistics Theory, Statistics - Machine Learning},
  Owner                    = {wittawat},
  Shorttitle               = {Kernels for Vector-Valued Functions},
  Timestamp                = {2015.01.23},
  Url                      = {http://arxiv.org/abs/1106.6251},
  Urldate                  = {2014-09-16}
}

@Article{Bach2003,
  Title                    = {Kernel Independent Component Analysis},
  Author                   = {Bach, Francis R. and Jordan, Michael I.},
  Journal                  = {J. Mach. Learn. Res.},
  Year                     = {2003},

  Month                    = mar,
  Pages                    = {1--48},
  Volume                   = {3},

  Acmid                    = {944920},
  Doi                      = {10.1162/153244303768966085},
  ISSN                     = {1532-4435},
  Issue_date               = {3/1/2003},
  Keywords                 = {Stiefel manifold, blind source separation, canonical correlations, gram matrices, incomplete Cholesky decomposition, independent component analysis, integral equations, kernel methods, mutual information, semiparametric models},
  Numpages                 = {48},
  Publisher                = {JMLR.org},
  Url                      = {http://dx.doi.org/10.1162/153244303768966085}
}

@InProceedings{Barthelme2011,
  Title                    = {ABC-EP: Expectation Propagation for Likelihood-free Bayesian Computation },
  Author                   = {Simon Barthelm\'{e} and Nicolas Chopin},
  Booktitle                = {Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
  Year                     = {2011},

  Address                  = {New York, NY, USA},
  Editor                   = {Lise Getoor and Tobias Scheffer},
  Month                    = {June},
  Pages                    = {289--296},
  Publisher                = {ACM},
  Series                   = {ICML '11},

  ISBN                     = {978-1-4503-0619-5},
  Location                 = {Bellevue, Washington, USA}
}

@Book{Bishop2006,
  Title                    = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
  Author                   = {Bishop, Christopher M.},
  Publisher                = {Springer-Verlag New York, Inc.},
  Year                     = {2006},

  Address                  = {Secaucus, NJ, USA},

  ISBN                     = {0387310738},
  Owner                    = {wittawat},
  Timestamp                = {2014.10.07}
}

@Article{Breiman2001,
  Title                    = {Random {Forests}},
  Author                   = {Breiman, Leo},
  Journal                  = {Mach. Learn.},
  Year                     = {2001},

  Month                    = oct,
  Number                   = {1},
  Pages                    = {5--32},
  Volume                   = {45},

  Abstract                 = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  Doi                      = {10.1023/A:1010933404324},
  ISSN                     = {0885-6125},
  Keywords                 = {classification, ensemble, regression},
  Owner                    = {wittawat},
  Timestamp                = {2015.02.23},
  Url                      = {http://dx.doi.org/10.1023/A:1010933404324},
  Urldate                  = {2015-02-23}
}

@Article{Caponnetto2007,
  Title                    = {Optimal Rates for the Regularized Least-Squares Algorithm},
  Author                   = {Caponnetto, A. and De Vito, E.},
  Journal                  = {Found. Comput. Math.},
  Year                     = {2007},

  Month                    = jul,
  Number                   = {3},
  Pages                    = {331--368},
  Volume                   = {7},

  Acmid                    = {1290534},
  Address                  = {Secaucus, NJ, USA},
  Doi                      = {10.1007/s10208-006-0196-8},
  ISSN                     = {1615-3375},
  Issue_date               = {July 2007},
  Numpages                 = {38},
  Owner                    = {wittawat},
  Publisher                = {Springer-Verlag New York, Inc.},
  Timestamp                = {2014.10.09},
  Url                      = {http://dx.doi.org/10.1007/s10208-006-0196-8}
}

@InProceedings{Christmann2010,
  Title                    = {Universal kernels on non-standard input spaces},
  Author                   = {Christmann, Andreas and Steinwart, Ingo},
  Booktitle                = {in Advances in Neural Information Processing Systems},
  Year                     = {2010},
  Pages                    = {406--414},

  Abstract                 = {During the last years support vector machines ({SVMs}) have been successfully applied in situations where the input space X is not necessarily a subset of R d. Examples include {SVMs} for the analysis of histograms or colored images, {SVMs} for text classification and web mining, and {SVMs} for applications from computational biology using, e.g., kernels for trees and graphs. Moreover, {SVMs} are known to be consistent to the Bayes risk, if either the input space is a complete separable metric space and the reproducing kernel Hilbert space ({RKHS}) H ⊂ Lp({PX}) is dense, or if the {SVM} uses a universal kernel k. So far, however, there are no kernels of practical interest known that satisfy these assumptions, if X ̸ ⊂ R d. We close this gap by providing a general technique based on Taylor-type kernels to explicitly construct universal kernels on compact metric spaces which are not subset of R d. We apply this technique for the following special cases: universal kernels on the set of probability measures, universal kernels based on Fourier transforms, and universal kernels for signal processing. 1},
  File                     = {Citeseer - Full Text PDF:/nfs/nhome/live/wittawat/.zotero/zotero/q6a3aco7.default/zotero/storage/PK3RTIX5/Christmann and Steinwart - 2010 - Universal kernels on non-standard input spaces.pdf:application/pdf;Citeseer - Snapshot:/nfs/nhome/live/wittawat/.zotero/zotero/q6a3aco7.default/zotero/storage/BBDWDS67/summary.html:text/html},
  Owner                    = {wittawat},
  Timestamp                = {2015.01.23}
}

@InProceedings{Eslami2014,
  Title                    = {{Just-In-Time Learning for Fast and Flexible Inference}},
  Author                   = {Eslami, S. M. Ali and Tarlow, Daniel and Kohli, Pushmeet and Winn, John},
  Booktitle                = {Advances in Neural Information Processing Systems 27},
  Year                     = {2014},
  Editor                   = {Ghahramani, Zoubin and Welling, Max and Cortes, Corinna and Lawrence, Neil D. and Weinberger, Kilian Q.},
  Pages                    = {154--162}
}

@Misc{Fukumizu2010,
  Title                    = {Kernel Bayes' rule},

  Author                   = {Fukumizu, Kenji and Song, Le and Gretton, Arthur},
  Year                     = {2010},

  Abstract                 = {A nonparametric kernel-based method for realizing Bayes' rule is proposed, based on representations of probabilities in reproducing kernel Hilbert spaces. Probabilities are uniquely characterized by the mean of the canonical map to the RKHS. The prior and conditional probabilities are expressed in terms of RKHS functions of an empirical sample: no explicit parametric model is needed for these quantities. The posterior is likewise an RKHS mean of a weighted sample. The estimator for the expectation of a function of the posterior is derived, and rates of consistency are shown. Some representative applications of the kernel Bayes' rule are presented, including Baysian computation without likelihood and filtering with a nonparametric state-space model.},
  Added-at                 = {2014-04-16T18:50:14.000+0200},
  Biburl                   = {http://www.bibsonomy.org/bibtex/2feb6401b9964b4cac3ee522b2878e4b5/wittawatj},
  Description              = {[1009.5736] Kernel Bayes' rule},
  Interhash                = {ec7f63bc48617e77473598da871cbb39},
  Intrahash                = {feb6401b9964b4cac3ee522b2878e4b5},
  Keywords                 = {embedding kernel},
  Timestamp                = {2014-04-16T18:50:14.000+0200},
  Url                      = {http://arxiv.org/abs/1009.5736}
}

@Article{Gelman2006,
  Title                    = {Prior distributions for variance parameters in hierarchical models},
  Author                   = {Gelman, Andrew},
  Journal                  = {Bayesian Analysis},
  Year                     = {2006},
  Pages                    = {1--19},
  Volume                   = {1},

  Abstract                 = {Abstract. Various noninformative prior distributions have been suggested for scale parameters in hierarchical models. We construct a new folded-noncentral-t family of conditionally conjugate priors for hierarchical standard deviation parameters, and then consider noninformative and weakly informative priors in this family. We use an example to illustrate serious problems with the inverse-gamma family of “noninformative ” prior distributions. We suggest instead to use a uniform prior on the hierarchical standard deviation, using the half-t family when the number of groups is small and in other settings where a weakly informative prior is desired. We also illustrate the use of the half-t family for hierarchical modeling of multiple variance parameters such as arise in the analysis of variance.},
  File                     = {Citeseer - Full Text PDF:/nfs/nhome/live/wittawat/.zotero/zotero/q6a3aco7.default/zotero/storage/576Z94H4/Gelman - 2006 - Prior distributions for variance parameters in hie.pdf:application/pdf},
  Owner                    = {wittawat},
  Timestamp                = {2015.02.17}
}

@Article{Geurts2006,
  Title                    = {Extremely {Randomized} {Trees}},
  Author                   = {Geurts, Pierre and Ernst, Damien and Wehenkel, Louis},
  Journal                  = {Mach. Learn.},
  Year                     = {2006},

  Month                    = apr,
  Number                   = {1},
  Pages                    = {3--42},
  Volume                   = {63},

  Abstract                 = {This paper proposes a new tree-based ensemble method for supervised classification and regression problems. It essentially consists of randomizing strongly both attribute and cut-point choice while splitting a tree node. In the extreme case, it builds totally randomized trees whose structures are independent of the output values of the learning sample. The strength of the randomization can be tuned to problem specifics by the appropriate choice of a parameter. We evaluate the robustness of the default choice of this parameter, and we also provide insight on how to adjust it in particular situations. Besides accuracy, the main strength of the resulting algorithm is computational efficiency. A bias/variance analysis of the Extra-Trees algorithm is also provided as well as a geometrical and a kernel characterization of the models induced.},
  Doi                      = {10.1007/s10994-006-6226-1},
  ISSN                     = {0885-6125},
  Keywords                 = {Bias/variance tradeoff, Cut-point randomization, Decision and regression trees, Ensemble methods, Kernel-based models, Supervised learning},
  Owner                    = {wittawat},
  Timestamp                = {2015.02.23},
  Url                      = {http://dx.doi.org/10.1007/s10994-006-6226-1},
  Urldate                  = {2015-02-23}
}

@InProceedings{Gruenewaelder2012,
  Title                    = {Conditional mean embeddings as regressors},
  Author                   = {Gr{\"u}new{\"a}lder, S. and Lever, G. and Baldassarre, L. and Patterson, S. and Gretton, A. and Pontil, M.},
  Booktitle                = {Proceedings of the 29th International Conference on Machine Learning},
  Year                     = {2012},

  Address                  = {New York, NY, USA},
  Editor                   = {Langford, J and Pineau, J},
  Pages                    = {1823--1830},
  Publisher                = {Omnipress},

  Department               = {Department Sch{\"o}lkopf},
  Event_name               = {ICML 2012},
  Event_place              = {Edinburgh, Scotland, GB},
  Url                      = {http://icml.cc/2012/papers/898.pdf},
  Web_url                  = {http://icml.cc/2012/papers/898.pdf}
}

@Article{Gretton2012,
  Title                    = {A Kernel Two-sample Test},
  Author                   = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Sch\"{o}lkopf, Bernhard and Smola, Alexander},
  Journal                  = {J. Mach. Learn. Res.},
  Year                     = {2012},

  Month                    = mar,
  Number                   = {1},
  Pages                    = {723--773},
  Volume                   = {13},

  Acmid                    = {2188410},
  Added-at                 = {2014-04-15T14:28:05.000+0200},
  Biburl                   = {http://www.bibsonomy.org/bibtex/2640187df9cfc28186b0b9a89c18fdd88/wittawatj},
  Interhash                = {58c9cf9277c7f55f7642b687f9a549f8},
  Intrahash                = {640187df9cfc28186b0b9a89c18fdd88},
  ISSN                     = {1532-4435},
  Issue_date               = {January 2012},
  Keywords                 = {kernel},
  Numpages                 = {51},
  Publisher                = {JMLR.org},
  Timestamp                = {2014-04-15T14:28:05.000+0200},
  Url                      = {http://dl.acm.org/citation.cfm?id=2503308.2188410}
}

@InCollection{Heess2013,
  Title                    = {Learning to Pass Expectation Propagation Messages},
  Author                   = {Nicolas Heess and Daniel Tarlow and John Winn},
  Booktitle                = {Advances in Neural Information Processing Systems 26},
  Year                     = {2013},
  Editor                   = {C.j.c. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.q. Weinberger},
  Pages                    = {3219--3227}
}

@PhdThesis{Hutter2009,
  Title                    = {Automated Configuration of Algorithms for Solving Hard Computational Problems},
  Author                   = {F. Hutter},
  School                   = {University of British Columbia, Department of Computer Science},
  Year                     = {2009},

  Address                  = {Vancouver, Canada},
  Month                    = {October},

  Lauthor                  = {Frank Hutter},
  Owner                    = {wittawat},
  Timestamp                = {2015.02.18}
}

@Article{Jampani2014,
  Title                    = {Consensus {Message} {Passing} for {Layered} {Graphical} {Models}},
  Author                   = {Jampani, Varun and Eslami, S. M. Ali and Tarlow, Daniel and Kohli, Pushmeet and Winn, John},
  Journal                  = {arXiv:1410.7452 [cs]},
  Year                     = {2014},

  Month                    = oct,
  Note                     = {arXiv: 1410.7452},

  Abstract                 = {Generative models provide a powerful framework for probabilistic reasoning. However, in many domains their use has been hampered by the practical difficulties of inference. This is particularly the case in computer vision, where models of the imaging process tend to be large, loopy and layered. For this reason bottom-up conditional models have traditionally dominated in such domains. We find that widely-used, general-purpose message passing inference algorithms such as Expectation Propagation (EP) and Variational Message Passing (VMP) fail on the simplest of vision models. With these models in mind, we introduce a modification to message passing that learns to exploit their layered structure by passing 'consensus' messages that guide inference towards good solutions. Experiments on a variety of problems show that the proposed technique leads to significantly more accurate inference results, not only when compared to standard EP and VMP, but also when compared to competitive bottom-up conditional models.},
  Annote                   = {Comment: Appearing in Proceedings of the 18th International Conference on Artificial Intelligence and Statistics (AISTATS) 2015},
  File                     = {arXiv\:1410.7452 PDF:/nfs/nhome/live/wittawat/.zotero/zotero/q6a3aco7.default/zotero/storage/F78JUNVX/Jampani et al. - 2014 - Consensus Message Passing for Layered Graphical Mo.pdf:application/pdf;arXiv.org Snapshot:/nfs/nhome/live/wittawat/.zotero/zotero/q6a3aco7.default/zotero/storage/KTCNGD2U/1410.html:text/html},
  Keywords                 = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
  Owner                    = {wittawat},
  Timestamp                = {2015.02.23},
  Url                      = {http://arxiv.org/abs/1410.7452},
  Urldate                  = {2015-02-23}
}

@InProceedings{Jebara2003,
  Title                    = {Bhattacharyya and Expected Likelihood Kernels},
  Author                   = {Tony Jebara and Risi Kondor},
  Booktitle                = {Conference on Learning Theory},
  Year                     = {2003},
  Publisher                = {press},

  Owner                    = {wittawat},
  Timestamp                = {2014.10.07}
}

@Article{Jebara2004,
  Title                    = {Probability Product Kernels},
  Author                   = {Jebara, Tony and Kondor, Risi and Howard, Andrew},
  Journal                  = {J. Mach. Learn. Res.},
  Year                     = {2004},

  Month                    = dec,
  Pages                    = {819--844},
  Volume                   = {5},

  Acmid                    = {1016786},
  ISSN                     = {1532-4435},
  Issue_date               = {12/1/2004},
  Numpages                 = {26},
  Owner                    = {wittawat},
  Publisher                = {JMLR.org},
  Timestamp                = {2014.10.04},
  Url                      = {http://dl.acm.org/citation.cfm?id=1005332.1016786}
}

@Article{Jylanki2011,
  Title                    = {Robust Gaussian Process Regression with a Student-{\it t} Likelihood},
  Author                   = {Pasi Jyl{\"a}nki and Jarno Vanhatalo and Aki Vehtari},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2011},
  Pages                    = {3227-3257},
  Volume                   = {12},

  Bibsource                = {DBLP, http://dblp.uni-trier.de},
  Ee                       = {http://dl.acm.org/citation.cfm?id=2078209}
}

@InProceedings{Le2013,
  Title                    = {Fastfood - Approximating Kernel Expansions in Loglinear Time},
  Author                   = {Quoc Le and Tamas Sarlos and Alex Smola},
  Booktitle                = {30th International Conference on Machine Learning (ICML)},
  Year                     = {2013},

  Url                      = {http://jmlr.org/proceedings/papers/v28/le13.html}
}

@Article{Micchelli2005,
  Title                    = {On Learning Vector-Valued Functions},
  Author                   = {Micchelli, Charles A. and Pontil, Massimiliano A.},
  Journal                  = {Neural Comput.},
  Year                     = {2005},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {177--204},
  Volume                   = {17},

  Abstract                 = {In this letter, we provide a study of learning in a Hilbert space of vectorvalued functions. We motivate the need for extending learning theory of scalar-valued functions by practical considerations and establish some basic results for learning vector-valued functions that should prove useful in applications. Specifically, we allow an output space Y to be a Hilbert space, and we consider a reproducing kernel Hilbert space of functions whose values lie in Y. In this setting, we derive the form of the minimal norm interpolant to a finite set of data and apply it to study some regularization functionals that are important in learning theory. We consider specific examples of such functionals corresponding to multiple-output regularization networks and support vector machines, for both regression and classification. Finally, we provide classes of operator-valued kernels of the dot product and translation-invariant type.},
  Doi                      = {10.1162/0899766052530802},
  ISSN                     = {0899-7667},
  Owner                    = {wittawat},
  Timestamp                = {2014.10.09},
  Url                      = {http://dx.doi.org/10.1162/0899766052530802},
  Urldate                  = {2014-10-09}
}

@Conference{Minka2002,
  Title                    = {{Expectation-Propagation for the Generative Aspect Model}},
  Author                   = {Minka, T.P. and Lafferty, John},
  Booktitle                = {Proceedings of the 18th Conference on Uncertainty in Artificial Intelligence},
  Year                     = {2002},
  Pages                    = {352--359},

  Added-at                 = {2010-03-25T16:35:09.000+0100},
  Biburl                   = {http://www.bibsonomy.org/bibtex/28e54f8b5b19fca4cbdeb6ae816867d71/3mta3},
  File                     = {minka2002.pdf:Papers/minka2002.pdf:PDF},
  Interhash                = {b4be319c9424a60c9d3bec9fe535b3ed},
  Intrahash                = {8e54f8b5b19fca4cbdeb6ae816867d71},
  Keywords                 = {Expectationpropagation Messagepassing LatentDirichletallocation},
  Timestamp                = {2010-03-25T16:35:09.000+0100},
  Url                      = {http://research.microsoft.com/en-us/um/people/minka/papers/aspect/}
}

@Misc{Minka2012,
  Title                    = {{Infer.NET 2.5}},

  Author                   = {Minka, T. and Winn, J.M. and Guiver, J.P. and Knowles, D.A.},
  Note                     = {Microsoft Research Cambridge. http://research.microsoft.com/infernet},
  Year                     = {2012},

  Owner                    = {wittawat},
  Timestamp                = {2014.10.08}
}

@PhdThesis{Minka2001,
  Title                    = {A Family of Algorithms for Approximate Bayesian Inference},
  Author                   = {Minka, Thomas P.},
  School                   = {Massachusetts Institute of Technology},
  Year                     = {2001},
  Note                     = {{AAI}0803033},

  Owner                    = {wittawat},
  Timestamp                = {2014.10.04}
}

@Misc{Nishiyama2012,
  Title                    = {Hilbert Space Embeddings of POMDPs},

  Author                   = {Nishiyama, Yu and Boularias, Abdeslam and Gretton, Arthur and Fukumizu, Kenji},
  Note                     = {cite arxiv:1210.4887Comment: Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence (UAI2012)},
  Year                     = {2012},

  Abstract                 = {A nonparametric approach for policy learning for POMDPs is proposed. The approach represents distributions over the states, observations, and actions as embeddings in feature spaces, which are reproducing kernel Hilbert spaces. Distributions over states given the observations are obtained by applying the kernel Bayes' rule to these distribution embeddings. Policies and value functions are defined on the feature space over states, which leads to a feature space expression for the Bellman equation. Value iteration may then be used to estimate the optimal value function and associated policy. Experimental results confirm that the correct policy is learned using the feature space representation.},
  Added-at                 = {2014-04-16T23:24:02.000+0200},
  Biburl                   = {http://www.bibsonomy.org/bibtex/23ff4535ad8efb8cf7d0247316a15326f/wittawatj},
  Description              = {[1210.4887] Hilbert Space Embeddings of POMDPs},
  Interhash                = {54a1a004fad42cb98ab00c6a7c7798fb},
  Intrahash                = {3ff4535ad8efb8cf7d0247316a15326f},
  Keywords                 = {kernel pomdp},
  Timestamp                = {2014-04-16T23:24:02.000+0200},
  Url                      = {http://arxiv.org/abs/1210.4887}
}

@Article{Poczos2013,
  Title                    = {Distribution-Free Distribution Regression},
  Author                   = {Poczos, Barnabas and Rinaldo, Alessandro and Singh, Aarti and Wasserman, Larry},
  Journal                  = {{arXiv}:1302.0082 [cs, math, stat]},
  Year                     = {2013},

  Month                    = feb,
  Note                     = {{arXiv}: 1302.0082},

  Abstract                 = {`Distribution regression' refers to the situation where a response Y depends on a covariate P where P is a probability distribution. The model is Y=f(P) + mu where f is an unknown regression function and mu is a random error. Typically, we do not observe P directly, but rather, we observe a sample from P. In this paper we develop theory and methods for distribution-free versions of distribution regression. This means that we do not make distributional assumptions about the error term mu and covariate P. We prove that when the effective dimension is small enough (as measured by the doubling dimension), then the excess prediction risk converges to zero with a polynomial rate.},
  File                     = {arXiv\:1302.0082 PDF:/nfs/nhome/live/wittawat/.zotero/zotero/q6a3aco7.default/zotero/storage/6R7XQIVI/Poczos et al. - 2013 - Distribution-Free Distribution Regression.pdf:application/pdf;arXiv.org Snapshot:/nfs/nhome/live/wittawat/.zotero/zotero/q6a3aco7.default/zotero/storage/KDKI6G45/1302.html:text/html},
  Keywords                 = {Computer Science - Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
  Owner                    = {wittawat},
  Timestamp                = {2014.10.04},
  Url                      = {http://arxiv.org/abs/1302.0082},
  Urldate                  = {2014-10-04}
}

@InProceedings{Rahimi2007,
  Title                    = {Random features for large-scale kernel machines},
  Author                   = {Rahimi, Ali and Recht, Ben},
  Booktitle                = {Neural Information Processing Systems},
  Year                     = {2007}
}

@Book{Scholkopf2002,
  Title                    = {Learning with kernels : support vector machines, regularization, optimization, and beyond},
  Author                   = {Sch\"{o}lkopf, Bernhard and Smola, Alexander J.},
  Publisher                = {MIT Press},
  Year                     = {2002},
  Series                   = {Adaptive computation and machine learning},

  Added-at                 = {2009-05-19T18:00:18.000+0200},
  Biburl                   = {http://www.bibsonomy.org/bibtex/2b453d5fc1a1cf2efdd085ff7e22b0050/earthfare},
  Citeulike-article-id     = {4545319},
  Description              = {CiteULike: Everyone's library},
  Interhash                = {a14f07242aaae08e3a1ed9abc06d99e6},
  Intrahash                = {b453d5fc1a1cf2efdd085ff7e22b0050},
  Keywords                 = {gaussian-process, kernel-design, kernel-method, machine-learning, rkhs},
  Owner                    = {wittawat},
  Posted-at                = {2009-05-19 16:31:20},
  Priority                 = {5},
  Timestamp                = {2009-05-19T18:00:18.000+0200}
}

@Article{Seeger2008,
  Title                    = {Bayesian inference and optimal design for the sparse linear model},
  Author                   = {Matthias Seeger},
  Journal                  = {The Journal of Machine Learning Research},
  Year                     = {2008},
  Pages                    = {759--813},
  Volume                   = {9},

  Biburl                   = {http://www.bibsonomy.org/bibtex/20266764a9d1a2ffee0a4037c87a2913f/andysend},
  Interhash                = {9b729cb0e972abea92ff56d998d31a93},
  Intrahash                = {0266764a9d1a2ffee0a4037c87a2913f},
  Publisher                = {JMLR. org},
  Timestamp                = {2014-01-17T09:53:33.000+0100}
}

@InProceedings{Smola2007,
  Title                    = {A Hilbert space embedding for distributions},
  Author                   = {Alex Smola and Arthur Gretton and Le Song and Bernhard Sch\"{o}lkopf},
  Booktitle                = {In Algorithmic Learning Theory: 18th International Conference},
  Year                     = {2007},
  Pages                    = {13--31},
  Publisher                = {Springer-Verlag},

  Owner                    = {wittawat},
  Timestamp                = {2014.10.07}
}

@Article{Song2013,
  Title                    = {Kernel Embeddings of Conditional Distributions: A Unified Kernel Framework for Nonparametric Inference in Graphical Models},
  Author                   = {Le Song and Kenji Fukumizu and Arthur Gretton},
  Journal                  = {IEEE Signal Process. Mag.},
  Year                     = {2013},
  Number                   = {4},
  Pages                    = {98-111},
  Volume                   = {30},

  Bibsource                = {DBLP, http://dblp.uni-trier.de},
  Ee                       = {http://dx.doi.org/10.1109/MSP.2013.2252713}
}

@InProceedings{Song2011,
  Title                    = {Kernel Belief Propagation.},
  Author                   = {Song, Le and Gretton, Arthur and Bickson, Danny and Low, Yucheng and Guestrin, Carlos},
  Booktitle                = {AISTATS},
  Year                     = {2011},
  Editor                   = {Gordon, Geoffrey J. and Dunson, David B. and Dudík, Miroslav},
  Pages                    = {707-715},
  Publisher                = {JMLR.org},
  Series                   = {JMLR Proceedings},
  Volume                   = {15},

  Added-at                 = {2013-11-25T00:00:00.000+0100},
  Biburl                   = {http://www.bibsonomy.org/bibtex/27b107600dae5183203f8b4d1e7968e44/dblp},
  Ee                       = {http://www.jmlr.org/proceedings/papers/v15/song11a/song11a.pdf},
  Interhash                = {33712c03100c4cc9693534da4a529e2b},
  Intrahash                = {7b107600dae5183203f8b4d1e7968e44},
  Keywords                 = {dblp},
  Timestamp                = {2013-11-25T00:00:00.000+0100}
}

@Article{Sriperumbudur2013,
  Title                    = {Density Estimation in Infinite Dimensional Exponential Families},
  Author                   = {Sriperumbudur, Bharath and Fukumizu, Kenji and Gretton, Arthur and Hyvarinen, Aapo},
  Journal                  = {{arXiv}:1312.3516 [math, stat]},
  Year                     = {2013},

  Month                    = dec,
  Note                     = {{arXiv}: 1312.3516},

  Annote                   = {Comment: 51 pages, 4 figures; this is a major revision of the first draft with additional results},
  Keywords                 = {Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Methodology},
  Owner                    = {wittawat},
  Timestamp                = {2014.10.07},
  Url                      = {http://arxiv.org/abs/1312.3516},
  Urldate                  = {2014-07-04}
}

@Misc{SDT2014,
  Title                    = {Stan: A C++ Library for Probability and Sampling, Version 2.4},

  Author                   = {{Stan Development Team}},
  Year                     = {2014},

  Owner                    = {wittawat},
  Timestamp                = {2014.10.08},
  Url                      = {http://mc-stan.org/}
}

@Article{Steinwart2001,
  Title                    = {On the Influence of the Kernel on the Consistency of Support Vector Machines},
  Author                   = {Ingo Steinwart},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2001},
  Pages                    = {67-93},
  Volume                   = {2},

  Bibsource                = {DBLP, http://dblp.uni-trier.de},
  Ee                       = {http://www.jmlr.org/papers/v2/steinwart01a.html},
  Owner                    = {wittawat},
  Timestamp                = {2014.02.07}
}

@Article{Szabo2014,
  Title                    = {Consistent, Two-Stage Sampled Distribution Regression via Mean Embedding},
  Author                   = {Szabo, Zoltan and Gretton, Arthur and Poczos, Barnabas and Sriperumbudur, Bharath},
  Year                     = {2014},

  Month                    = feb,

  File                     = {Full Text PDF:/nfs/nhome/live/wittawat/.zotero/zotero/q6a3aco7.default/zotero/storage/U73GQN5G/Szabo et al. - 2014 - Consistent, Two-Stage Sampled Distribution Regress.pdf:application/pdf;Snapshot:/nfs/nhome/live/wittawat/.zotero/zotero/q6a3aco7.default/zotero/storage/FG3D22SH/1402.html:text/html},
  Owner                    = {wittawat},
  Timestamp                = {2014.10.04},
  Url                      = {http://arxiv-web3.library.cornell.edu/abs/1402.1754v2},
  Urldate                  = {2014-10-04}
}

@Article{Wainwright2008,
  Title                    = {Graphical Models, Exponential Families, and Variational Inference},
  Author                   = {Martin J. Wainwright and Michael I. Jordan},
  Journal                  = {Foundations and Trends in Machine Learning},
  Year                     = {2008},
  Number                   = {1-2},
  Pages                    = {1-305},
  Volume                   = {1},

  Bibsource                = {DBLP, http://dblp.uni-trier.de},
  Ee                       = {http://dx.doi.org/10.1561/2200000001}
}

@InCollection{Yedidia2003,
  Title                    = {Exploring Artificial Intelligence in the New Millennium},
  Author                   = {Yedidia, Jonathan S. and Freeman, William T. and Weiss, Yair},
  Publisher                = {Morgan Kaufmann Publishers Inc.},
  Year                     = {2003},

  Address                  = {San Francisco, CA, USA},
  Chapter                  = {Understanding Belief Propagation and Its Generalizations},
  Editor                   = {Lakemeyer, Gerhard and Nebel, Bernhard},
  Pages                    = {239--269},

  Acmid                    = {779352},
  ISBN                     = {1-55860-811-7},
  Numpages                 = {31},
  Url                      = {http://dl.acm.org/citation.cfm?id=779343.779352}
}

@InProceedings{Zelnik-manor2004,
  Title                    = {Self-tuning spectral clustering},
  Author                   = {Zelnik-manor, Lihi and Perona, Pietro},
  Booktitle                = {Advances in Neural Information Processing Systems 17},
  Year                     = {2004},
  Pages                    = {1601-1608},
  Publisher                = {{MIT} Press}
}

@comment{jabref-meta: selector_publisher:}

@comment{jabref-meta: selector_author:}

@comment{jabref-meta: selector_journal:}

@comment{jabref-meta: selector_keywords:}

